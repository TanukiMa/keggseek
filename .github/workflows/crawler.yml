name: ÂéöÂä¥ÁúÅ„Çµ„Ç§„ÉàÂ∞ÇÈñÄÁî®Ë™ûËß£Êûê

on:
  schedule:
    # ÊØéÊó•ÂçàÂâç2ÊôÇÔºàJST 11ÊôÇÔºâ„Å´ÂÆüË°å
    - cron: '0 2 * * *'
  workflow_dispatch:  # ÊâãÂãïÂÆüË°å„ÇÇÂèØËÉΩ
    inputs:
      max_workers:
        description: '‰∏¶ÂàóÂá¶ÁêÜÊï∞'
        required: false
        default: '3'
        type: string

env:
  PYTHON_VERSION: '3.11'
  SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
  SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}

jobs:
  analyze-mhlw-terms:
    runs-on: ubuntu-latest
    timeout-minutes: 120  # 2ÊôÇÈñì„Åß„Çø„Ç§„É†„Ç¢„Ç¶„Éà
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          openjdk-11-jre-headless \
          build-essential \
          cmake \
          pkg-config
        
        # JavaÁí∞Â¢ÉÂ§âÊï∞Ë®≠ÂÆö
        echo "JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64" >> $GITHUB_ENV
        echo "PATH=$PATH:/usr/lib/jvm/java-11-openjdk-amd64/bin" >> $GITHUB_ENV
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install \
          requests==2.31.0 \
          beautifulsoup4==4.12.2 \
          supabase==2.0.2 \
          sudachipy==0.6.7 \
          huggingface_hub==0.19.4 \
          python-docx==0.8.11 \
          python-pptx==0.6.22 \
          PyPDF2==3.0.1 \
          lxml==4.9.3
    
    - name: Download Sudachi dictionary
      run: |
        mkdir -p ~/.sudachi
        # SudachiËæûÊõ∏„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„ÉâÔºàÂÖ¨Âºè„É™„Éù„Ç∏„Éà„É™„Åã„ÇâÔºâ
        wget -O ~/.sudachi/system.dic \
          https://object-storage.tyo2.conoha.io/v1/nc_2520839e1f9641b08211a5c85243124a/sudachi/sudachi-dictionary-20230110/system_core.dic
        
        # Ë®≠ÂÆö„Éï„Ç°„Ç§„É´‰ΩúÊàê
        cat > ~/.sudachi/sudachi.json << 'EOF'
        {
          "systemDict": "~/.sudachi/system.dic",
          "characterDefinitionFile": "char.def",
          "inputTextPlugin": [],
          "oovProviderPlugin": [],
          "pathRewritePlugin": [],
          "connectPlugin": []
        }
        EOF
    
    - name: Install Hugging Face CLI and llama.cpp (from Debian sid)
      run: |
        pip install huggingface_hub
        
        # Debian sid (unstable) „É™„Éù„Ç∏„Éà„É™„ÇíËøΩÂä†„Åó„Å¶llama.cpp„Çí„Ç§„É≥„Çπ„Éà„Éº„É´
        echo "‚ÑπÔ∏è  Adding Debian sid repository for llama.cpp package"
        
        # sid„É™„Éù„Ç∏„Éà„É™ËøΩÂä†ÔºàÊúÄÂ∞èÈôê„ÅÆÂΩ±Èüø„Å´ÈôêÂÆöÔºâ
        echo "deb http://deb.debian.org/debian sid main" | sudo tee /etc/apt/sources.list.d/sid.list
        
        # ÂÑ™ÂÖàÂ∫¶Ë®≠ÂÆöÔºàsid„Åã„Çâ„ÅØÂøÖË¶Å„Å™„Éë„ÉÉ„Ç±„Éº„Ç∏„ÅÆ„ÅøÔºâ
        sudo tee /etc/apt/preferences.d/llama-cpp-pin << 'EOF'
        Package: llama.cpp llama.cpp-tools
        Pin: release a=sid
        Pin-Priority: 500
        
        Package: *
        Pin: release a=sid
        Pin-Priority: 100
        EOF
        
        # „Éë„ÉÉ„Ç±„Éº„Ç∏„É™„Çπ„ÉàÊõ¥Êñ∞„Å®„Ç§„É≥„Çπ„Éà„Éº„É´
        sudo apt update
        sudo apt install -y llama.cpp
        
        # Âãï‰ΩúÁ¢∫Ë™ç
        llama-cli --help | head -5
        echo "LLAMA_CLI_PATH=llama-cli" >> $GITHUB_ENV
    
    - name: Download LLM model with huggingface-cli
      run: |
        mkdir -p models
        
        # Êó•Êú¨Ë™ûÂØæÂøú„ÅÆËªΩÈáè„É¢„Éá„É´„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ
        huggingface-cli download \
          mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf \
          ELYZA-japanese-Llama-2-7b-fast-instruct-q4_0.gguf \
          --local-dir models \
          --local-dir-use-symlinks False
        
        # „É¢„Éá„É´„Éï„Ç°„Ç§„É´Âêç„ÇíÁµ±‰∏Ä
        mv models/ELYZA-japanese-Llama-2-7b-fast-instruct-q4_0.gguf models/ggml-model-Q4_K_M.gguf
        
        echo "LLAMA_MODEL_PATH=$(pwd)/models/ggml-model-Q4_K_M.gguf" >> $GITHUB_ENV
        echo "LLAMA_CLI_PATH=/usr/local/bin/llama-cli" >> $GITHUB_ENV
    
    - name: Create cache directories
      run: |
        mkdir -p ~/.cache/sudachi
        mkdir -p /tmp
    
    - name: Verify environment
      run: |
        python --version
        java -version
        echo "üß™ llama-cli version and capabilities:"
        llama-cli --version || llama-cli --help | head -3
        echo "SUPABASE_URL: ${SUPABASE_URL:0:20}..."
        echo "Model path: $LLAMA_MODEL_PATH"
        echo "CLI path: $LLAMA_CLI_PATH"
        ls -la models/
        file models/ggml-model-Q4_K_M.gguf
        
        # „Éë„ÉÉ„Ç±„Éº„Ç∏„ÇΩ„Éº„ÇπÁ¢∫Ë™ç
        echo "üì¶ Installed llama.cpp package info:"
        dpkg -l | grep llama || echo "Package info not found"
    
    - name: Initialize Supabase dictionary
      run: |
        python3 << 'EOF'
        import os
        from supabase import create_client
        
        # Âü∫Êú¨ËæûÊõ∏„Éá„Éº„Çø„ÇíÊäïÂÖ•ÔºàÂàùÂõû„ÅÆ„ÅøÔºâ
        client = create_client(os.environ['SUPABASE_URL'], os.environ['SUPABASE_KEY'])
        
        # Êó¢Â≠òËæûÊõ∏ÂçòË™û„ÅÆ‰æãÔºàÂÆüÈöõ„Å´„ÅØ„ÇÇ„Å£„Å®Â§ßÈáè„Å´ÊäïÂÖ•Ôºâ
        basic_words = [
            {'word': 'ÂåªÁôÇ', 'reading': '„Ç§„É™„Éß„Ç¶', 'part_of_speech': 'ÂêçË©û', 'source': 'basic'},
            {'word': 'ÂéöÁîüÂä¥ÂÉçÁúÅ', 'reading': '„Ç≥„Ç¶„Çª„Ç§„É≠„Ç¶„Éâ„Ç¶„Ç∑„Éß„Ç¶', 'part_of_speech': 'ÂêçË©û', 'source': 'basic'},
            {'word': 'ÂÅ•Â∫∑', 'reading': '„Ç±„É≥„Ç≥„Ç¶', 'part_of_speech': 'ÂêçË©û', 'source': 'basic'},
        ]
        
        try:
            # Êó¢Â≠ò„ÉÅ„Çß„ÉÉ„ÇØ
            existing = client.table('dictionary_words').select('word').limit(1).execute()
            if not existing.data:
                client.table('dictionary_words').insert(basic_words).execute()
                print("Âü∫Êú¨ËæûÊõ∏„ÇíÂàùÊúüÂåñ„Åó„Åæ„Åó„Åü")
            else:
                print("ËæûÊõ∏„ÅØÊó¢„Å´ÂàùÊúüÂåñÊ∏à„Åø„Åß„Åô")
        except Exception as e:
            print(f"ËæûÊõ∏ÂàùÊúüÂåñ„Ç®„É©„Éº: {e}")
        EOF
    
    - name: Run crawler analysis
      env:
        MAX_WORKERS: ${{ github.event.inputs.max_workers || '3' }}
      run: |
        python3 << 'EOF'
        import sys
        import os
        sys.path.append('.')
        
        # „É°„Ç§„É≥„ÇØ„É≠„Éº„É©„ÉºÂÆüË°å
        try:
            from main_crawler import MhlwCrawler
            crawler = MhlwCrawler()
            max_workers = int(os.environ.get('MAX_WORKERS', '3'))
            crawler.run(max_workers=max_workers)
            print("‚úÖ „ÇØ„É≠„Éº„É©„ÉºÂÆüË°åÂÆå‰∫Ü")
        except Exception as e:
            print(f"‚ùå „Ç®„É©„Éº: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
        EOF
    
    - name: Generate summary report
      if: always()
      run: |
        python3 << 'EOF'
        import os
        from supabase import create_client
        from datetime import datetime, timedelta
        
        client = create_client(os.environ['SUPABASE_URL'], os.environ['SUPABASE_KEY'])
        
        # ‰ªäÊó•„ÅÆÁµêÊûú„Çµ„Éû„É™„Éº
        today = datetime.now().date()
        
        # Êñ∞Ë™ûÂÄôË£úÊï∞
        new_words = client.table('new_word_candidates')\
            .select('*')\
            .gte('created_at', today.isoformat())\
            .execute()
        
        # Âá¶ÁêÜURLÊï∞
        processed = client.table('processed_urls')\
            .select('*')\
            .gte('created_at', today.isoformat())\
            .execute()
        
        print(f"""
        üìä ÂÆüË°åÁµêÊûú„Çµ„Éû„É™„Éº
        ==================
        Êó•ÊôÇ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
        Âá¶ÁêÜÊ∏àURL: {len(processed.data)}‰ª∂
        Êñ∞Ë™ûÂÄôË£ú: {len(new_words.data)}‰ª∂
        
        üîç Êñ∞Ë™ûÂÄôË£ú„Éà„ÉÉ„Éó5:
        """)
        
        for i, word in enumerate(new_words.data[:5], 1):
            print(f"{i}. {word['word']} ({word['reading']}) - ‰ø°È†ºÂ∫¶: {word['confidence_score']:.2f}")
        
        print("\n‚úÖ Ëß£ÊûêÂÆå‰∫Ü")
        EOF
    
    - name: Cleanup temporary files
      if: always()
      run: |
        rm -rf /tmp/*.pdf /tmp/*.docx /tmp/*.pptx
        rm -rf models/  # Â§ß„Åç„ÅÑ„ÅÆ„ÅßÂâäÈô§ÔºàÊ¨°ÂõûÂÜç„ÉÄ„Ç¶„É≥„É≠„Éº„ÉâÔºâ
    
    - name: Upload logs (on failure)
      if: failure()
      uses: actions/upload-artifact@v4
      with:
        name: crawler-logs-${{ github.run_id }}
        path: |
          *.log
          /tmp/crawler_*.txt
        retention-days: 7
