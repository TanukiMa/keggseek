# .github/workflows/crawler.yml
name: New Word Crawler Pipeline (Parallel)

on:
  schedule:
    - cron: '0 18 * * *' # 毎日午前3時 (JST)
  workflow_dispatch:

jobs:
  # Job 1: URLを発見してキューに追加する係
  discover:
    runs-on: ubuntu-latest
    steps:
      - name: 1. Checkout repository
        uses: actions/checkout@v4

      - name: 2. Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: 3. Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 supabase

      - name: 4. Run URL Discoverer
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        run: |
          python discover_urls.py

  # Job 2: キューを処理してコンテンツを解析する係
  process:
    # `discover`ジョブとは独立して並列実行される
    runs-on: ubuntu-latest
    
    # === ここがポイント: 解析ジョブを複数同時に実行する ===
    strategy:
      matrix:
        # この数字を増やすと、解析ワーカーの数が増える
        instance: [1, 2, 3] 
    
    steps:
      - name: 1. Checkout repository
        uses: actions/checkout@v4

      - name: 2. Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: 3. Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install sudachipy SudachiDict-full
          pip install requests beautifulsoup4 pdfplumber python-docx python-pptx supabase

      - name: 4. Run Queue Processor (Instance ${{ matrix.instance }})
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        run: |
          python process_queue.py
